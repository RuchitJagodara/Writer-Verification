{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import os\n",
    "import torchvision\n",
    "from torch.nn import functional as F\n",
    "import torch\n",
    "import random\n",
    "import argparse, random, copy\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms as T\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file = \"dataset/dataset/train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SiameseNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SiameseNN, self).__init__()\n",
    "        self.resnet = torchvision.models.resnet18(pretrained=False)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.fc_in_features = self.resnet.fc.in_features\n",
    "        self.resnet = nn.Sequential(*list(self.resnet.children())[:-1])\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(2*self.fc_in_features, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.resnet.apply(self.init_weights)\n",
    "        self.fc.apply(self.init_weights)\n",
    "    \n",
    "    def init_weights(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "            m.bias.data.fill_(0.01)\n",
    "    \n",
    "    def forward_once(self, x):\n",
    "        output = self.resnet(x)\n",
    "        output = output.view(output.size()[0], -1)\n",
    "        return output\n",
    "\n",
    "    def forward(self, inp1, inp2):\n",
    "        ouput1 = self.forward_once(inp1)\n",
    "        ouput2 = self.forward_once(inp2)\n",
    "        output = torch.cat((ouput1, ouput2), 1)\n",
    "        output = self.fc(output)\n",
    "        output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(v1, v2, margin=1):\n",
    "    scores = torch.matmul(v1, v2.t())\n",
    "    class_size = scores.size(-2)\n",
    "    positive = scores.diagonal(dim1=-2, dim2=-1)\n",
    "    negative_without_positive = scores - torch.eye(class_size).to(scores.device) * 2\n",
    "    negative = torch.max(negative_without_positive, dim=-1)[0]\n",
    "    negative_zero_on_duplicate = scores * (1 - torch.eye(class_size).to(scores.device))\n",
    "    mean_negative = negative_zero_on_duplicate.sum(dim=-1) / (class_size - 1)\n",
    "    triplet_loss1 = torch.clamp(margin + negative - positive, min=0)\n",
    "    triplet_loss2 = torch.clamp(margin + mean_negative - positive, min=0)\n",
    "    triplet_loss = triplet_loss1.mean() + triplet_loss2.mean()\n",
    "    return triplet_loss.mean()\n",
    "\n",
    "transform = T.Resize((120, 2000))\n",
    "ind = 0\n",
    "\n",
    "def train(img_path_pair, label, model, loss_fn, optimizer):\n",
    "    global ind\n",
    "    print(\"Training\", ind) if (ind%1000 == 0) else None\n",
    "    ind += 1\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    img1_path, img2_path = img_path_pair\n",
    "    img1 = Image.open(img1_path)\n",
    "    img1 = transform(img1)\n",
    "    img2 = Image.open(img2_path)\n",
    "    img2 = transform(img2)\n",
    "    label = torch.tensor(label).float().to(device)\n",
    "    label = label.reshape(1, 1)\n",
    "    img1 = torch.tensor(np.array(img1)).float().unsqueeze(0).to(device)\n",
    "    img2 = torch.tensor(np.array(img2)).float().unsqueeze(0).to(device)\n",
    "    img1 = img1.reshape(1, 1, 120, 2000)\n",
    "    img2 = img2.reshape(1, 1, 120, 2000)\n",
    "    output = model(img1, img2)\n",
    "    loss = loss_fn(output, label)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruchitjagodara/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ruchitjagodara/.local/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "model = SiameseNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_pairs = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fld in os.listdir(data_file):\n",
    "\n",
    "    img_set = os.listdir(data_file+\"/\"+fld)\n",
    "    for img in img_set:\n",
    "        for img2 in img_set:\n",
    "            if (img!=img2):\n",
    "                img_pairs.append([data_file+\"/\"+fld+\"/\"+img, data_file+\"/\"+fld+\"/\"+img2])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'P577/B3.jpg'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_pairs[0][0][22:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check(path1, path2):\n",
    "    if (path1[22:].split('/')[0]==path2[22:].split('/')[0]):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 0\n",
      "Training 1000\n",
      "Training 2000\n",
      "Training 3000\n",
      "Training 4000\n",
      "Training 5000\n",
      "Training 6000\n",
      "Training 7000\n",
      "Training 8000\n",
      "Training 9000\n",
      "Training 10000\n",
      "Training 11000\n",
      "Training 12000\n",
      "Training 13000\n",
      "Training 14000\n",
      "Training 15000\n",
      "Training 16000\n",
      "Training 17000\n",
      "Training 18000\n",
      "Training 19000\n",
      "Training 20000\n",
      "Training 21000\n",
      "Training 22000\n",
      "Training 23000\n",
      "Training 24000\n",
      "Training 25000\n",
      "Training 26000\n",
      "Training 27000\n",
      "Training 28000\n",
      "Training 29000\n",
      "Training 30000\n",
      "Training 31000\n",
      "Training 32000\n",
      "Training 33000\n",
      "Training 34000\n",
      "Training 35000\n",
      "Training 36000\n",
      "Training 37000\n",
      "Training 38000\n",
      "Training 39000\n",
      "Training 40000\n",
      "Training 41000\n",
      "Training 42000\n",
      "Training 43000\n",
      "Training 44000\n",
      "Training 45000\n",
      "Training 46000\n",
      "Training 47000\n",
      "Training 48000\n",
      "Training 49000\n",
      "Training 50000\n",
      "Training 51000\n",
      "Training 52000\n",
      "Training 53000\n",
      "Training 54000\n",
      "Training 55000\n",
      "Training 56000\n",
      "Training 57000\n",
      "Training 58000\n",
      "Training 59000\n",
      "Training 60000\n",
      "Training 61000\n",
      "Training 62000\n",
      "Training 63000\n",
      "Training 64000\n",
      "Training 65000\n",
      "Training 66000\n",
      "Training 67000\n",
      "Training 68000\n",
      "Training 69000\n",
      "Training 70000\n",
      "Training 71000\n",
      "Training 72000\n",
      "Training 73000\n",
      "Training 74000\n",
      "Training 75000\n",
      "Training 76000\n",
      "Training 77000\n",
      "Training 78000\n",
      "Training 79000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image_pair \u001b[38;5;129;01min\u001b[39;00m img_pairs:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_pair\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtriplet_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mAdam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0001\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m     new_img \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(img_pairs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m(check(new_img, image_pair[\u001b[38;5;241m0\u001b[39m])):\n",
      "Cell \u001b[0;32mIn[32], line 36\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(img_path_pair, label, model, loss_fn, optimizer)\u001b[0m\n\u001b[1;32m     34\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(output, label)\n\u001b[1;32m     35\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 36\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:385\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    381\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    382\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    383\u001b[0m             )\n\u001b[0;32m--> 385\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    388\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:157\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    154\u001b[0m     state_steps \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    155\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 157\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_group\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    159\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m     adam(\n\u001b[1;32m    167\u001b[0m         params_with_grad,\n\u001b[1;32m    168\u001b[0m         grads,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    186\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    187\u001b[0m     )\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/optim/adam.py:108\u001b[0m, in \u001b[0;36mAdam._init_group\u001b[0;34m(self, group, params_with_grad, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# Lazy state initialization\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(state) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# note(crcrpar): [special device hosting for step]\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# Deliberately host `step` on CPU if both capturable and fused are off.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# This is because kernel launches are costly on CUDA and XLA.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    106\u001b[0m         torch\u001b[38;5;241m.\u001b[39mzeros((), dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32, device\u001b[38;5;241m=\u001b[39mp\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mor\u001b[39;00m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 108\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m     )\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# Exponential moving average of gradient values\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mexp_avg\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(p, memory_format\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mpreserve_format)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for image_pair in img_pairs:\n",
    "    train(image_pair, 1, model, triplet_loss, optim.Adam(model.parameters(), lr=0.0001))\n",
    "    new_img = random.choice(img_pairs)[0]\n",
    "    while(check(new_img, image_pair[0])):\n",
    "        new_img = random.choice(img_pairs)[0]\n",
    "    train((new_img, image_pair[0]), 0, model, triplet_loss, optim.Adam(model.parameters(), lr=0.0001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(img_path_pair, label, model):\n",
    "    img1_path, img2_path = img_path_pair\n",
    "    img1 = Image.open(img1_path)\n",
    "    img1 = transform(img1)\n",
    "    img2 = Image.open(img2_path)\n",
    "    img2 = transform(img2)\n",
    "    label = torch.tensor(label).float().to(device)\n",
    "    label = label.reshape(1, 1)\n",
    "    img1 = torch.tensor(np.array(img1)).float().unsqueeze(0).to(device)\n",
    "    img2 = torch.tensor(np.array(img2)).float().unsqueeze(0).to(device)\n",
    "    img1 = img1.reshape(1, 1, 120, 2000)\n",
    "    img2 = img2.reshape(1, 1, 120, 2000)\n",
    "    output = model(img1, img2)\n",
    "    if (output==label):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(img_pairs[0], 1, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
